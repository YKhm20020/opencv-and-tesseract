# はじめに

本システムは、スマートフォンで撮影した紙媒体の書類フォームをデジタル化する、というコンセプトのもとで作成している卒業研究のものである。  
具体的には、入力画像を二値化した後に矩形領域を取得するという内容である。

# 実行環境
- OS: Window8, Windows10

# 矩形領域を抽出する機能

export_array という名前のファイルは、矩形領域抽出に関するシステムである。詳細は、各 export_array ディレクトリの README.md を参照。  
以下では、簡単に実行方法と出力画像の説明を行う。export_array については、矩形領域のみを抽出する export_array と、矩形領域に加え、下線部を認識するよう拡張した export_array_boxline の2つに分けて説明する。  

## export_array について

export_array という名前のファイルは、矩形領域抽出に関するシステムである。詳細は、export_array ディレクトリの README.md を参照。  
以下では、簡単に実行方法と出力画像の説明を行う。

Dockerfile によってコンテナ作成後、`python3 export_array.py`で export_array.py を実行する。実行後、img.png という.png ファイルが生成されると同時に、ターミナルに番号と 4 つの数字が出力される。  
出力画像にはランダムの色で取得した矩形領域の枠が色付けられ、同じ色で番号が振られる。この番号は、ターミナルに出力された番号と一致する。  
4 つの数字は、左上を原点として、対象の矩形領域について[x座標 y座標]というように、各頂点の座標を示している。
~~左下、左上、右上、右下の順が多いが、一部は左上、右上、右下、左下の順で出力されているときもある。~~  
最新バージョンにて、左上、左下、右下、右上と、左上から反時計回りに座標が並ぶよう修正済み。以下はターミナルにおける出力の例である。

```
rect(0):
 [[2231  205]
 [2606  199]
 [2614  705]
 [2239  711]]
rect(1):
 [[ 284  366]
 [2110  351]
 [2111  454]
 [ 285  469]]
 .
 .
 .
```

## export_array_boxline.py について
矩形領域の取得する export_array に加え、export_array_boxline では下線部検出の機能を備えており、拡張に成功している。   
実行方法は、他ファイルと同じである。export_array_boxline.pyは、 `python3 export_array_boxline.py` のコマンドで実行することができる。

実行後、ターミナルに rect, line の数字と、img.png, img2.png の2つの .png ファイルが生成される。  

rect の数字については、export_array 実行時と同様のものであるため、省略。

line の数字については、水平線として認識した線の両端の座標を示しており、line(数字): [左端x座標, 左端y座標, 右端x座標, 右端y座標] のように出力される。line(数字) の数字については、後述する img2.png で書かれている数字と一致している。数字の順番は、左端x座標が小さい順となっている。

実際のターミナルの出力は、以下のようになる。

```
rect(0):
 [[ 891   57]
 [ 891  109]
 [1131  109]
 [1131   57]]
rect(1):
 [[ 798  739]
 [ 798  844]
 [1131  844]
 [1131  739]]

 ・
 ・
 ・
rect(9):
 [[ 491 1575]
 [ 491 1594]
 [ 695 1594]
 [ 695 1575]]

line(0): [352 463 857 463]
line(1): [352 532 857 532]
line(2): [375 221 857 221]
line(3): [376 290 857 290]
line(4): [476 834 772 834]
line(5): [ 709  152 1112  152]

```

img.png は、export_array で実装した、取得した矩形領域を入力画像に加えた画像。  
img2.png は、水平線のみを検出し、入力画像に緑色の線で書き加えた画像である。矩形領域の上下辺に水平線を含むが、これは検出対象から除外している。   
なお rect と同じく、line(数字) の数字と、img2.png の線付近に書かれている数字は一致する。

# 文字を抽出する機能

OCR という名前のファイルは、光学文字認識 (Optical Character Recognition) に関するシステムである。詳細は、各 OCR ディレクトリの README.md を参照。  
以下では、簡単に実行方法と出力画像の説明を行う。OCRについては、Tesseract-OCR を用いたものと、PaddleOCR を用いたものの2つに分けて説明する。  

## Tesseract-OCR について
最初に、Tesseract-OCR を用いた OCR.py についての説明を行う。詳細は、OCR/Tesseract ディレクトリの README.md を参照。

Dockerfile によってコンテナ作成後、`python3 OCR.py`で OCR.py を実行する。実行後、img_OCR.png という.png ファイルが生成されると同時に、ターミナルに文字と 4 つの数字が出力される。  
出力画像に認識した文字枠が赤色で色付けられる。4 つの数字は、対象の赤枠の左上の点のx, y座標、右下の点のx, y座標の順にそれぞれの角が何ピクセルにあたるかを示している。わかりやすいよう、最後に抽出した文字のみを再表示している。以下は、ターミナルにおける出力の例である。  
なお、最初の OCR ツールの表示は削除予定である。

```
Tesseract (sh)
Tesseract (C-API)
履 歴
((279, 225), (504, 317))
圭
((557, 222), (646, 279))
過

.
.
.

横24一30mm

2.本人単身胸から上

氏名

3.裏面のりづけ

ふりがな
```

## PaddleOCR について
次に、PaddleOCR を用いた OCR_paddle.py, OCR_paddle_color.py についての説明を行う。詳細は、OCR/PaddleOCR ディレクトリの README.md を参照。  
両者ファイルとも PaddleOCR を用いて文字とその位置を抽出しているが、前者は入力画像に対して二値化処理を行った上で抽出を行うファイルであり、後者は入力画像そのままに対して抽出を行うファイルである。  
実行のコマンドは、それぞれ`python3 OCR_paddle.py`と、`python3 OCR_paddle_color.py`である。

Dockerfile によってコンテナ作成後、上のコマンドで OCR_paddle.py, OCR_paddle_color を実行する。実行後、img_OCR_paddle.png または img_OCR_paddle_color という.png ファイルが生成されると同時に、ターミナルに文字と 4 つの数字に加え、0~1 の浮動小数点数が出力される。  
出力画像に認識した文字枠が赤色で色付けられる。4 つの数字は、対象の赤枠の左上の点のx, y座標、右下の点のx, y座標の順にそれぞれの角が何ピクセルにあたるかを示している。確認用として、最後に抽出した文字とその位置を再表示している。このときの左の数字は、出力画像における矩形領域付近にある数字と一致している。浮動小数点数は、抽出した文字が合っているかどうかを示したものの推論の値である。この値が高ければ高いほど、精度が高くなる傾向にある。  
以下は、ターミナルにおける出力の例である。なお、二者ファイル間で出力そのものは変化するが、出力の形式は統一している。  
また、実行時にファイルのインストールが入るが、これは実行に必要なモデルをインストールしているためである。詳細は OCR/Paddle ディレクトリの README.md を参照。
なお、以下で出力された文字列には、普通に読むと誤字脱字等とみなされるものがあるが、これは私のタイプミスによるものではなく、文字抽出の誤りであり、抽出した内容をそのままペーストしている。  

```
履歴書
(277, 224, 651, 318)
0.9105887413024902
年月日
(1432, 256, 1764, 307)
0.9936679005622864
現在
(1819, 254, 1914, 304)
0.9999313354492188
写真を貼る位置
(2295, 276, 2542, 309)
0.9993799924850464

・
・
・

0: 履歴書 (277, 224, 651, 318)
1: 年月日 (1432, 256, 1764, 307)
2: 現在 (1819, 254, 1914, 304)
・
・
・
32: [性別 (311, 3797, 437, 3839)
33: 欄：記載は任意です (464, 3796, 879, 3839)
34: 未記載とすることも可能です。 (912, 3799, 1517, 3826)
```


## export_array_ex, OCR_ex について
export_array_ex.py という名前のファイルは、 export_array と同じく矩形領域を検出する機能に加え、新たにその矩形領域ごとに画像を切り取る機能を追加したものである。  
現段階ではまだ使うことはないが、調査したところ、画像を文字ごとに切り取ると OCR の精度がより向上するようなので、先に実装を進めた。  
しかし、Tesseract-OCR にこの処理を適応させた OCR_ex.py を作成・実行したものの、精度は明確に低下した。　
PaddleOCR については、もともとの精度が高く、矩形領域内のみ精度を向上させたとして、入力画像によっては矩形領域の数が少ないなど、恩恵が少ないことが考えられるため、あまり役に立たないという見込みである。

# 属性付与について
MeCab を使用して日本語のデータセットを分かち書きし、それを fastText でテキスト分類するという機能を現在開発中である。  
これは、OCRで得た文字に対して、それがどの型にあたるのかを推測するという動作を期待している。例えば、「氏名」には char, 「生年月日」であれば date, 「ある、なし」であれば boolean、といった具合だ。  

しかし、データセットを自作し、テキスト分類を実行したが、全ての文字が char 型であると分類されてしまっている。現時点で、主に2つの原因があると推測している。

- データの不足  
現在、450個程度のデータしかないため、分類に必要な情報が足りていない可能性がきわめて高い

- データの品質  
データセットを自作しているため、品質は非常に悪い。データの偏り、似た単語の登場などが原因であると推測している。

なお、属性付与実行の方法としては、以下の順でファイルを実行するとよい。

1. `python3 bert/old_version/csv_to_owakati_txt.py` コマンドで、データセット train_data.csv を訓練データ train.tsv, テストデータ test.tsv に分割。それら2つを対象に、ラベル名の右にある日本語テキストを MeCab で分かち書きする。

2. `python3 bert/old_version/train_supervised.py` コマンドで、分かち書きしたデータから学習モデルを生成、`ret = model.predict('<分類対象のテキスト>')` の行で対象のテキストを指定し、データにその型がいくつあるかと、分類結果を表示する。

## train_keras_bert.py について
train_keras_bert は、Keras BERT を用いた文書分類を、既にあるデータセットをファインチューニングすることで学習モデルをつくるファイル。現在作成中。

以下のコマンドを順に実行することで、正しく動作する。

1. `python3 bert/data/label_feature_split.py` コマンドで、ルートディレクトリにある train_data.csv を参照し、'label,feature' の一行を追加した train_bert_data.csv を bert/data に作成。同時に、label 列とtrain 列を分割した CSV ファイル2つを、tests, trains の2つディレクトリに作成する。(それぞれ labels_test.csv, labels_train.csv, features_test.csv, features_train.csv の計4つ)

2. `python3 train_keras_bert.py` コマンドで、訓練・テストデータをもとに学習済みモデルをファインチューニング。処理後、モデルの評価を行う。予測まで行いたい場合は、このコマンドをスキップ可。

3. `python3 prediction.py` コマンドで、ファインチューニング後のモデルに対し、予測を行う。2. のコマンドも同時実行。